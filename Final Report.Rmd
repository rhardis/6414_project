---
title: "Wines - Final Report"
author: "Richard Hardis, Paulina Jane LoCicero, James Trawick"
date: "12/1/2019"
output:
  word_document: default
  html_document: default
toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# ISYE 6414 Final Project: Regression Analysis of Wines

## Summary
***
  Our goal is to evaluate factors that influence wine quality and price, based on a dataset collected from https://www.kaggle.com/zynicide/wine-reviews, which was scraped from WineEnthusiast in June of 2017, then again in November 2017 (https://www.winemag.com/?s=&drink_type=wine).  We augmented this dataset with several other geographical and textual predictors to help further explore the drivers of wine price and quality.  Winery latitude, longitude, and elevation were gathered from the the mapquest Open APIs.  Temperature and Precipitation data for countries was gathered from the WorldBank’s open climate API. The foundational winemag.com data set contains information for 130,000 different wines, some of the factors include: country, description, and designation among others.  Length of review was also calculated for each wine to use as a potentially significant predictor.  Due to API limitations, we are randomly sampling 2000 wines from the original 130,000 to use for data cleaning and then build our model. With this data, we hope to answer the following questions:
Under which conditions does higher wine rating correspond with higher reviews?  Specifically, with which predicting variables included in a model does that model show rating being a statistically significant predictor of price? How accurate is a multiple linear regression model built off of the winemag.com 2017 dataset at predicting the prices of the top 20 wines of 2019 according to totalwine.com? Do some regions produce wines with higher quality (how does region influence points awarded?). We intend to run an initial simple linear regression, a multiple linear regression, an ANOVA test, and will evaluate other models which may fit the data well based on our initial findings. Finally, our team will also explore robust regression methodologies (lasso, elastic net, etc…) in order to identify important attributes and reduce overfitting in for our predictive models.

## Background
***
### Study Motivations
  We want to know more about wine...
  
### Study Expectations
  Wine price and quality are related and can be explained better with the help of other variables
  
## Data
***
### Raw Data and Collection
  Our raw data started from a root dataset of wines taken from https://www.kaggle.com/zynicide/wine-reviews as part of their machine learning competition.  This dataset contains wine data for over 130,000 distinct wines and was scraped from the search results pages of winemag.com in June and November of 2017.  The original dataset contained country, description, designation, points (quality score), price, province (state in the US or province in Europe), region_1, region_2 (subregion of the province or region_1), taster_name, taster_twitter_handle, title, variety, and winery.  This dataset had many data points, but did not have many useful predictors for regression analysis, so we chose to scrape further webpages to gather more factors.  Two datasets were developed with additional factors.  The first large dataset has predictors  review length (word count), year produced, abv (alcohol by volume), bottle_size, category (red, white, rose), and importer scraped from the individual wine page on winemag.com.  This large dataset has 73,256 individual wines.  The second, smaller dataset contains all of the features of the large dataset and also includes average country temperature, standard deviation of country temperature, average country precipitation, standard deviation of country precipitation, lattitude, longitude, and elevation.  There are 1123 unique wines in this smaller dataset.  Winery latitude, longitude, and elevation were gathered from the the mapquest Open APIs.  Temperature and Precipitation data for countries was gathered from the WorldBank’s open climate API.  The large quantity of data requires detailed exploration to ensure its quality and suitability for regression analysis.
  
### Data Exploration
  The first step in the data exploration is cleaning. This involved removing any wines that did not have data for price, quality, year,temperature, precipitation, elevation, review length, abv, bottle size, or importer.  Because of the large quantity of data available, this approach of removing wines with missing data is preferable to imputing using averages or other means.  Next, data exploration of the data can commence.
  
```{r echo=FALSE}
# Read in the data
setwd("~/GitHub/6414_project/")
large_data = read.csv("datasets/large_wines_nonan.csv")
small_data = read.csv("datasets/small_wines_nonan.csv")
```

```{r echo=FALSE}
# subset the data columns
large_data = large_data[,c(1,4,5,6,14,16,17,18,19)]
large_data$category = as.factor(large_data$category)
small_data = small_data[,c(1,4,5,6,11,21,22,23,25,26)]
small_data$category = as.factor(small_data$category)
```

  The first step in exploring the data is checking how the data are distrubted and how each factor varies with all other factors.  Upon basic exploration it can be found that the variable Bottle Size does not vary as all wines were judged in 750ml bottles. This variable is removed because it does not vary.  Next, outliers need to be examined and dealt with.

First, box plots will be used to find outliers.  Outliers are then removed. For example, wines with alcohol contents above 50%, prices above $200, and year produced outside of the years 2000-2020 were removed from the data. Additionally, only red, white, and rose wines are considered.

```{r, echo=FALSE, warning=FALSE}
library(dplyr)
abv_limit = 50
small_data = small_data %>% filter(abv<abv_limit & price<200 & year>2000 & year<2020 & elevation>0  & (category=='Red' | category=='White' | category=='Rose'))
large_data = large_data %>% filter(abv<abv_limit & price<200 & year>2000 & year<2020 & (category=='Red' | category=='White' | category=='Rose'))

library(cowplot)
library(ggplot2)
# Basic box plot
p1 = ggplot(small_data, aes(x=category, y=price)) + 
  geom_boxplot()+theme(text = element_text(size=11), axis.text.x = element_text(angle=90, hjust=1))
p2 = ggplot(large_data, aes(x=category, y=price)) + 
  geom_boxplot()+theme(text = element_text(size=11), axis.text.x = element_text(angle=90, hjust=1))
p3 = ggplot(small_data, aes(x=category, y=points)) + 
  geom_boxplot()+theme(text = element_text(size=11), axis.text.x = element_text(angle=90, hjust=1))
p4 = ggplot(large_data, aes(x=category, y=points)) + 
  geom_boxplot()+theme(text = element_text(size=11), axis.text.x = element_text(angle=90, hjust=1))
p5 = ggplot(small_data, aes(x=category, y=elevation)) + 
  geom_boxplot()+theme(text = element_text(size=11), axis.text.x = element_text(angle=90, hjust=1))

plot_grid(p1, p2, p3, p4, p5, labels = c('Small', 'Large','Small', 'Large', 'Small'), label_size = 10)
```


```{r}
attach(small_data)
par(mfrow = c(2, 3))
hist(points, main = "Histogram of Points", xlab = "Wine Score", col = 2)
hist(price, main = "Histogram of Price", xlab = "Wine Price", col = 3)
hist(abv, main = "Histogram of Alcohol by Volume", xlab = "ABV (%)", col = 5)
hist(year, main = "Histogram of Year Produced", xlab = "Year", col = 8)
hist(review_length, main = "Histogram of Review Length", xlab = "Word Count", col = 9)

attach(large_data)
par(mfrow = c(2, 3))
hist(points, main = "Histogram of Points", xlab = "Wine Score", col = 2)
hist(price, main = "Histogram of Price", xlab = "Wine Price", col = 3)
hist(abv, main = "Histogram of Alcohol by Volume", xlab = "ABV (%)", col = 5)
hist(year, main = "Histogram of Year Produced", xlab = "Year", col = 8)
hist(review_length, main = "Histogram of Review Length", xlab = "Word Count", col = 9)
```



```{r}
library(GGally)
#ggpairs(large_data[,c(-1,-4,-7)], progress = FALSE)
```

```{r}
ggpairs(small_data[,c(-1,-4,-10)], progress = FALSE)
```

  Cook's distance for each dataset is examined next.
  
```{r}
model1 = lm(points~., data=small_data[,c(-1,-4,-10)])
cook = cooks.distance(model1)
plot(cook, type="h", lwd=3, col="red", ylab = "Cook's Distance", main="Cook's Distance")
sdcook = cbind(small_data, cook)
sdcook2 = sdcook %>% filter(cook<.005)
sdcook2 = sdcook2[,c(-1,-4)]
model2 = lm(points~.,data=sdcook2)
cutoff = 4/(nrow(small_data))
plot(model2, which=4, cook.levels=cutoff)
print(cutoff)
sdcook2
```

```{r}
#model3 = lm(points~., data=large_data[,c(-1,-4,-7)])
#cook = cooks.distance(model3)
#plot(cook, type="h", lwd=3, col="red", ylab = "Cook's Distance", main="Cook's Distance")
#ldcook = cbind(large_data, cook)
#ldcook2 = ldcook %>% filter(cook<.005)
#ldcook2 = ldcook2[,c(-1,-4,-10)]
#model4 = lm(points~.,data=ldcook2)
#cutoff = 4/(nrow(large_data))
#plot(model4, which=4, cook.levels=cutoff)
```

```{r}
small_data = sdcook2[,c(-9)]
fullData = small_data


set.seed(190)
testRows = sample(nrow(fullData),0.1*nrow(fullData))
testData = fullData[testRows, ]
trainData = fullData[-testRows, ]


levels(trainData$importer)[levels(trainData$importer)=="not_imported"] = "0"
levels(trainData$importer)[levels(trainData$importer)!="0"] = "1"
levels(testData$importer)[levels(testData$importer)=="not_imported"] = "0"
levels(testData$importer)[levels(testData$importer)!="0"] = "1"

```


## Modeling Analyses
***
### Model Approach 1: Multiple Linear Regression
  MLR with variable selection and metric analyses (r^2, adj. r^2, assumptions, etc.)
  
1) Exploratory Data Analysis

a) Boxplot of the qualitiative predictors and the response. 

```{r}

p3 = ggplot(small_data, aes(x=category, y=points)) + 
  geom_boxplot()+theme(text = element_text(size=11), axis.text.x = element_text(angle=90, hjust=1))
p4 = ggplot(large_data, aes(x=category, y=points)) + 
  geom_boxplot()+theme(text = element_text(size=11), axis.text.x = element_text(angle=90, hjust=1))


plot_grid(p3, p4, labels = c('Small', 'Large'), label_size = 10)

```

There does appear to be a relationship between Category (Red, Rose, White) and Points. Red is associated with the largest amount of points, White is associated with the second most and rose is associated with the fewest amount of points. The first, second and third quartile values for Red are approximately 87, 88.5 and 91 respectively. The first, second and third quartile values for Rose are approximately 85, 87 and 88.5 respectively. The first, second and third quartile values for White are approximately 86, 88 and 90. 


```{r}

p4 = ggplot(small_data, aes(x=importer, y=points)) + 
  geom_boxplot()+theme(text = element_text(size=11), axis.text.x = element_text(angle=90, hjust=1))


plot_grid(p4, labels = 'Small', label_size = 10)
```


```{r}
#levels(large_data$importer)[levels(large_data$importer)!="not_imported"] = "imported"
```

There does not appear to be a significant relationship between the imported predictor (imported yes or no) on points. However we will still keep this predictor as an a priori controlling factor, before variable selection. 

b) Scatterplot matrix of the quantitative predictors 
```{r}
ggpairs(large_data[,c(-4,-5)], progress = FALSE)
```

c) Is the linear model reasonable?

The linear model seems like a reasonable model so far. The quantitative predictor review length has a high (.538) correlation with the response, and the relationship appears linear. The quantitatve predictor price also has a high (.533) correlation with the response. The relationship is modeled well as a linear one, however the log transformation of price seems appropriate here:

```{r}
plot(large_data$points, log(large_data$price))
cor(large_data$points, log(large_data$price))

```


2) Fitting an MLR Model


```{r}
mlrmodel1 = lm(points~log(price) + abv + category + importer + year + review_length, data = large_data)
summary(mlrmodel1)
```


```{r}
confint(mlrmodel1, level = .99)
```
```{r}
confint(mlrmodel1, level = .95)
```

All predictors and the intercept are significantly non-zero at the 0.99 level except for ABV. ABV is significantly non zero at the 0.95 level.

Rose and and not imported are significantly negative, when compared to their respevtive baselines (Red and imported). All other predictors are significantly positive, with White being significantly positive when compared to Red. 

3) Checking Model Assumptions

a) Scatterplot of the standardized residuals versus the fitted values and a QQ  Plot and Histogram of the residuals. 

```{r}
library(car)
model1 = mlrmodel1
resids = rstandard(model1)
fits = model1$fitted


par(mfrow =c(2,2))
plot(fits, resids, xlab="Fitted Values", ylab="Residuals", main="Scatterplot")

qqPlot(resids, ylab="Residuals", main = "QQ Plot")
hist(resids, xlab="Residuals", main = "Histogram")
```

Constant Variance Assumption:  The constant variance assumption, as determined by the "Scatterplot" plot, holds somewhat well here. The variance is mostly uniform, except for at very low and very high fitted values. 

Normality Assumption: The normality assumption, as determined by the "Histogram" and "QQ Plot" plots, holds very well here. 

b) VIF for each predictor

```{r}
library(car)
vif(model1)
```
VIF is given in the first column. All predictors in the model have a VIF lower than max(10,1/(1-Rsq))=max(10,1/(1-0.5092))=10 indicating multicollinearity is not present in our model.

4) Model/Variable Selection


a) Full Model MLR

  i) Mallow's CP, AIC, and BIC
  
```{r}
mod1 = model1
library(CombMSC)
n=nrow(large_data)
fullmodelscores = data.frame("Mallows" = Cp(mod1,S2=summary(mod1)$sigma^2), "AIC" = AIC(mod1,k=2),"BIC" = AIC(mod1,k=log(n)))
fullmodelscores

```
  

b) Forward Stepwise MLR

   i) Fitting Forward Stepwise
   
```{r}
reducedmodel =lm(points~1,data=large_data)
forwardmlr = step(reducedmodel, scope = list(lower=reducedmodel,upper=mod1), direction = "forward",trace=F)
summary(forwardmlr)
```
    ii) i) Mallow's CP, AIC, and BIC
    
```{r}
mod1 = forwardmlr
library(CombMSC)
n=nrow(large_data)
forwardmodelscores = data.frame("Mallows" = Cp(mod1,S2=summary(mod1)$sigma^2), "AIC" = AIC(mod1,k=2),"BIC" = AIC(mod1,k=log(n)))
forwardmodelscores
```
    

c) Lasso MLR

   i) Fitting a Lasso 
   
```{r}
library(glmnet)

predictors = model.matrix(points ~ ., data = large_data)

response = data.matrix(large_data[, "points"])

lassomodel.cv = cv.glmnet(predictors, response, alpha=1, nfolds=10)

lassomodel = glmnet(predictors, response, alpha=1, nlambda = 100)

lassomodel.cv$lambda.min

coef(lassomodel,s=lassomodel.cv$lambda.min)



```
   
  ii) Plotting Coefficient Path
  
```{r}
plot(lassomodel,xvar="lambda",label=TRUE,lwd=2)
abline(v=log(lassomodel.cv$lambda.min),col='black',lty = 2,lwd=2)
```
  

 iii) Mallows, AIC, and BIC

d) Elastic Net

 i) Fitting an Elastic Net
 
```{r}


elasticmodel.cv = cv.glmnet(predictors, response, alpha=.5, nfolds=10)

elasticmodel = glmnet(predictors, response, alpha=.5, nlambda = 100)

elasticmodel.cv$lambda.min

coef(elasticmodel,s=elasticmodel.cv$lambda.min)
```
 
 
 
ii) plotting the coefficient path


```{r}
plot(elasticmodel,xvar="lambda",label=TRUE,lwd=2)
abline(v=log(elasticmodel.cv$lambda.min),col='black',lty = 2,lwd=2)
```



### Model Approach 2: Poisson Regression
  Poisson model with variable selection and metric analyses (r^2, adj. r^2, GOF, assumptions, etc.)

## Conclusions
***
### Implications
  What did we find?

### Further Questions
  What's next?
